<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="Open-Sans.css">
  <link rel="stylesheet" href="index.css">
  <title></title>
  <script defer="defer" src="./static/js/main.cb41f6a5.js"></script>
  <link href="./static/css/main.4017e162.css" rel="stylesheet">
  <meta name="description"
        content="FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation">
  <title>FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation</title>
</head>

<body>
  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://loopyavatar.github.io/">
                        Loopy
                    </a>
                    <a class="navbar-item" href="https://cyberhost.github.io/">
                        CyberHost
                    </a>
                </div>
            </div>
        </div>

    </div>
  </nav> -->

  <div id="root" class="column-flex">
    <div id="title-flex" class="column-flex">
      <h1> FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation </h1>
      <span>
        <a target="_blank" href="" onclick="return false;">Tianyun&nbsp;Zhong</a><sup>1*‡</sup>,
        <a target="_blank" href="" onclick="return false;">Chao&nbsp;Liang</a><sup>2*</sup>,
        <a target="_blank" href="" onclick="return false;">Jianwen&nbsp;Jiang</a><sup>2*†</sup>,
        <a target="_blank" href="" onclick="return false;">Gaojie&nbsp;Lin</a><sup>2</sup>,
        <a target="_blank" href="" onclick="return false;">Jiaqi&nbsp;Yang</a><sup>2</sup>,
        <a target="_blank" href="" onclick="return false;">Zhou&nbsp;Zhao</a><sup>1</sup>
        <br />
      </span>
      <span><sup>1</sup>Zhejiang&nbsp;University,<sup>2</sup>Bytedance</span>
      <span><sup>*</sup>Equal contribution,<sup>†</sup>Project lead,<sup>‡</sup>Internship at Bytedance</span>
      <div class="flex flex-gap" style="margin-bottom:0.5em;">
        <!-- <a target="_blank" href="" ><button>Paper</button></a> -->
	      <a target="_blank" href="" onclick="alert('Coming Soon!');return false;"><button>Paper</button></a>
        <a target="_blank" href="https://fadavatar.github.io"><button>Page</button></a>
      </div>
      <p>
        FADA-Balanced (4.17X speedup)
      </p>
      <div class="video-slider">
        <video src="video/balanced_22.mp4"></video>
        <video src="video/balanced_0.mp4"></video>
        <video src="video/balanced_11.mp4"></video>
        <video src="video/balanced_8.mp4"></video>
        <video src="video/balanced_26.mp4"></video>
      </div>
      <p>
        FADA-Fast (12.5X speedup)
      </p>
      <div class="video-slider">
        <video src="video/fast_22.mp4"></video>
        <video src="video/fast_0.mp4"></video>
        <video src="video/fast_11.mp4"></video>
        <video src="video/fast_8.mp4"></video>
        <video src="video/fast_26.mp4"></video>
      </div>

      <h3>Abstract</h3>

      <small><span>Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose <b>FADA</b> (<b>FA</b>st <b>D</b>iffusion <b>A</b>vatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times.</span></small>
      <div class='responsive-image-container'>
        <img src='image/overall.png' alt='' />
      </div>
    </div>

    <div id="sections" class="column-flex">
      <h3>Comparison with Recent Methods</h3>
        <div class="video-container">
          <video controls playsInline src="video/comparison_4.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/comparison_3.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/comparison_1.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/comparison_2.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/comparison_5.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/comparison_6.mp4"></video>
        </div>

      <h3>Visualizations of CFG Control Abilities with Multi-CFG Distillation</h3>
        <div class="video-container">
          <video controls playsInline src="video/ref_cfg_comparison.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/audio_cfg_comparison.mp4"></video>
        </div>

      <h3>Ethics Concerns</h3>
        <p>
          The purpose of this work is only for research. The images and audios used in these demos are from public sources. If there are any concerns, please contact us (zhongtianyun@zju.edu.cn) and we will delete it in time. The template of this webpage is from <a href="https://github.com/vasavatar/VASA-1">VASA-1</a>.
        </p>

      <!-- <h3>Acknowledgement</h3>
        <p>
          Some figures about film and interview in <b>More Video Results</b> are from <a href="https://celebv-hq.github.io/">Celebv-HQ</a>. Some test audios and images are from <a href="https://badtobest.github.io/echomimic.html">Echomimic</a>, <a href="https://humanaigc.github.io/emote-portrait-alive/">EMO</a>, and <a href="https://www.microsoft.com/en-us/research/project/vasa-1">VASA-1</a> et al. Thanks to these great work!
        </p> -->
      
      <!-- <h3>BibTeX</h3>
        <p>If you find this project is useful to your research, please cite us:</p>
        <pre><code>
          @article{jiang2024loopy,
            title={Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency},
            author={Jiang, Jianwen and Liang, Chao and Yang, Jiaqi and Lin, Gaojie and Zhong, Tianyun and Zheng, Yanbo},
            journal={arXiv preprint arXiv:2409.02634},
            year={2024}
          }
        </code></pre> -->

      <br/>
      <br/>
      <br/>
    </div>
  </div>
  <script src="index.js"></script>
  <script>
    function comming_soon_click() {
      alert('Comming soon!');
    }
    function TBD_click() {
      alert('TBD');
    }
  </script>
</body>



</html>
